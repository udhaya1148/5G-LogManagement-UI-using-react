
# Configuration for telegraf agent
#[agent]
#  interval = "30s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
#  round_interval = true

  ## This controls the size of writes that Telegraf sends to output plugins.
#  metric_batch_size = 1000

  ## This buffer only fills when writes fail to output plugin(s).
#  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
#  collection_jitter = "2s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
#  flush_interval = "30s"
#  flush_jitter = "5s"

  ## By default or when set to "0s", precision will be set to the same
#  precision = ""

  ## Logging configuration:
#  debug = true
  ## Run telegraf in quiet mode (error log messages only).
#  quiet = false
  ## Specify the log file name. The empty string means to log to stderr.
#  logfile = ""

  ## Override default hostname, if empty use os.Hostname()
#  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
#  omit_hostname = true



[agent]
  interval = "30s"
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 10000
  collection_jitter = "0s"
  flush_interval = "30s"
  flush_jitter = "5s"
  precision = ""
  debug = true
  quiet = false
  logfile = ""







# Read metrics from one or more commands that can output to stdout
[[inputs.exec]]
  ## Commands array
  commands = ["python3 /etc/telegraf/custom_script/logs/amf-logs.py"]
  timeout = "10s"
  data_format = "json"
  json_string_fields = ["*"]


[[inputs.exec]]
  ## Commands array
  commands = ["python3 /etc/telegraf/custom_script/logs/smf-logs.py"]
  timeout = "10s"
  data_format = "json"
  json_string_fields = ["*"]

[[inputs.exec]]
  ## Commands array
  commands = ["python3 /etc/telegraf/custom_script/logs/upf-logs.py"]
  timeout = "10s"
  data_format = "json"
  json_string_fields = ["*"]
  

#[[inputs.exec]]
#  commands = ["your_command_here"]
#  data_format = "log"

#[[inputs.grok]]
#  namepass = ["exec"]
#  patterns = ["%{TIMESTAMP_ISO8601:timestamp:ts-\"2006-01-02T15:04:05Z07:00\"} %{WORD:measurement_name} %{WORD:field1} %{WORD:field2}"]
#  measurement = "parsed_measurement"
#  custom_patterns = '''
#    TIMESTAMP_ISO8601 %{TIMESTAMP_ISO8601}
#  '''



#[inputs.logparser.grok]
    ## This is a list of patterns to check the given log file(s) for.
    ## Note that adding patterns here increases processing time. The most
    ## efficient configuration is to have one pattern per logparser.
    ## Other common built-in patterns are:
    ##   %{COMMON_LOG_FORMAT}   (plain apache & nginx access logs)
    ##   %{COMBINED_LOG_FORMAT} (access logs + referrer & agent)
#    patterns = ["%{COMBINED_LOG_FORMAT}"]

    ## Name of the outputted measurement name.
#    measurement = "apache_access_log"

    ## Full path(s) to custom pattern files.
#    custom_pattern_files = []

    ## Custom patterns can also be defined here. Put one pattern per line.
#    custom_patterns = '''
#    '''

    ## Timezone allows you to provide an override for timestamps that
    ## don't already include an offset
    ## e.g. 04/06/2016 12:41:45 data one two 5.43Âµs
    ##
    ## Default: "" which renders UTC
    ## Options are as follows:
    ##   1. Local             -- interpret based on machine localtime
    ##   2. "Canada/Eastern"  -- Unix TZ values like those found in https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
    ##   3. UTC               -- or blank/unspecified, will return timestamp in UTC
#     timezone = "UTC"

    ## When set to "disable", timestamp will not incremented if there is a
    ## duplicate.
    # unique_timestamp = "auto"






#[[outputs.file]]
#  files = ["/path/to/parsed_logs.txt"]
#  data_format = "influx"











  
[[outputs.influxdb_v2]]
  ## The URLs of the InfluxDB cluster nodes.
  ##
  ## Multiple URLs can be specified for a single cluster, only ONE of the
  ## urls will be written to each interval.
  ## urls exp: http://127.0.0.1:9999
  urls = ["http://192.168.1.200:8086"]

  ## Token for authentication.
  token = "_OmCYJ26koWHXUkP9E2iZbCIXkP5_e5SgnYzugKOuNF6Z-xA1zRGwJ6BtOClFkxp7u59lAuh0EH0eLWJYHlNhg=="

  ## Organization is the name of the organization you wish to write to; must exist.
  organization = "dept-5g"

  ## Destination bucket to write into.
  bucket = "core"

#[[processors.regex]]
#  namepass = ["your_exec_input_name"]
#  regex = "(?<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) (?<measurement_name>\\S+) (?<field1>\\S+) (?<field2>\\S+)"
#  measurement = "parsed_measurement"
#  fieldpass = ["field1", "field2"]

#[[outputs.file]]
#  files = ["/home/aether/parsed_logs/parsed_logs.txt"]
#  data_format = "influx"

